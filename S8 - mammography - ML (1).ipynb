{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7602952c6772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPool2D, Flatten\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "from os import getcwd, chdir, mkdir\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "from matplotlib.pyplot import imread \n",
    "from pylab import ginput\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "from scipy.ndimage import convolve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1 = pd.read_csv(\"D:/mias-mammography/Info.txt\")\n",
    "dataframe1.to_csv(\"./Info.csv\", \n",
    "                  index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec=pd.read_csv(\"../input/mias-mammography/Info.txt\",sep=\" \")\n",
    "data_spec=data_spec.drop('Unnamed: 7',axis=1)\n",
    "data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppression des mammographies dites \"Saines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec.dropna(subset = [\"SEVERITY\"], inplace=True) \n",
    "data_spec.reset_index(inplace = True)\n",
    "data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec=data_spec.drop([3], axis=0)\n",
    "data_spec.reset_index(inplace = True)\n",
    "data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspec = {}\n",
    "for i in range(len(data_spec)):\n",
    "    dspec[i] = data_spec.REFNUM[i]\n",
    "dspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correspondance Bégigne, Maligne vers 1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = []\n",
    "for i in range(len(data_spec)):\n",
    "    if data_spec.SEVERITY[i] == 'B':\n",
    "        lb.append(1)\n",
    "    else:\n",
    "        lb.append(0)\n",
    "lb=np.array(lb)\n",
    "img_name = []\n",
    "path = 'D:/mias-mammography/all-mias/'\n",
    "for i in range(len(lb)):\n",
    "        img_name.append(path + data_spec.REFNUM[i]+ '.pgm')\n",
    "img_name = np.array(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_25_random_image():\n",
    "    fig = plt.figure(figsize = (15, 10))\n",
    "    for i in range(25):\n",
    "        rand = random.randint(0,len(lb))\n",
    "        ax = plt.subplot(5, 5, i+1)\n",
    "    \n",
    "        img = cv2.imread(img_name[rand], 0)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        if lb[rand] == 1:\n",
    "            plt.title('B')\n",
    "        else:\n",
    "            plt.title('M')\n",
    "        plt.tight_layout()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    fig.savefig('random_25_image_fig.png')\n",
    "\n",
    "random_images = view_25_random_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture et Zoom des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = []\n",
    "last_lb = []\n",
    "for i in range(len(img_name)):\n",
    "    \n",
    "    img = cv2.imread(img_name[i], 0)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    rows, cols= img.shape\n",
    "    for angle in range(360):\n",
    "            M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)    #Rotate 0 degree\n",
    "            img_rotated = cv2.warpAffine(img, M, (224, 224))\n",
    "            img_path.append(img_rotated)\n",
    "            if lb[i] == 1:\n",
    "                last_lb.append(1)\n",
    "            else:\n",
    "                last_lb.append(0)\n",
    "last_lb = np.array(last_lb)\n",
    "img_path = np.array(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation Train/Test du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(img_path, last_lb, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "(a,b,c)=x_train.shape # (35136, 224, 224)\n",
    "x_train = np.reshape(x_train, (a, b, c, 1)) # 1 for gray scale\n",
    "(a, b, c)=x_test.shape\n",
    "x_test = np.reshape(x_test, (a, b, c, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation de la quantité de données (Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorFlow keras preprocessing image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition et création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same',activation='relu',input_shape=(224, 224, 1)))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), padding = 'same',activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), padding = 'same',activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "  \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=0,restore_best_weights=True, verbose=1)\n",
    "\n",
    "check_point_filepath = './'\n",
    "\n",
    "model_check_point = ModelCheckpoint(filepath =check_point_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                    save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(datagen.flow(x_train, y_train, batch_size=32,\n",
    "                 subset='training'),\n",
    "                 steps_per_epoch = len(x_train)/32,\n",
    "                 validation_split=0.3,\n",
    "                 epochs=5,\n",
    "                 batch_size=8,\n",
    "                 callbacks=[early_stop, model_check_point],validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "def saveModel(model, savename):\n",
    "  # serialize model to YAML\n",
    "  model_yaml = model.to_yaml()\n",
    "  with open(savename+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "    print(\"Yaml Model \",savename,\".yaml saved to disk\")\n",
    "  # serialize weights to HDF5\n",
    "  model.save_weights(savename+\".h5\")\n",
    "  print(\"Weights \",savename,\".h5 saved to disk\")\n",
    "saveModel(model, save1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions de visualisation de la précision et de la perte du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value , accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test_loss_value = ' +str(loss_value))\n",
    "print('test_accuracy = ' + str(accuracy))\n",
    "\n",
    "print(model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize(acc,val_acc,loss, val_loss):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n",
    "                                   ncols = 2,\n",
    "                                   figsize = (15,6),\n",
    "                                   sharex =True)\n",
    "\n",
    "    plot1 = ax1.plot(range(0, len(acc)),\n",
    "                     acc,\n",
    "                     label = 'accuracy')\n",
    "\n",
    "    plot2 = ax1.plot(range(0, len(val_acc)),\n",
    "                     val_acc,\n",
    "                     label = 'val_accuracy')\n",
    "\n",
    "    ax1.set(title = 'Accuracy And Val Accuracy progress',\n",
    "            xlabel = 'epoch',\n",
    "            ylabel = 'accuracy/ validation accuracy')\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    plot3 = ax2.plot(range(0, len(loss)),\n",
    "                     loss,\n",
    "                     label = 'loss')\n",
    "    \n",
    "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
    "                     val_loss,\n",
    "                     label = 'val_loss')\n",
    "    \n",
    "    ax2.set(title = 'Loss And Val loss progress',\n",
    "            xlabel = 'epoch',\n",
    "            ylabel = 'loss/ validation loss')\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n",
    "    fig.savefig('Accuracy_Loss_figure.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result = Visualize(hist.history['accuracy'],hist.history['val_accuracy'], hist.history['loss'], hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test,batch_size=16,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des filtres gaussiens et de Butterworth qui vont être appliqués au dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtpb_gauss(A, fc):\n",
    "    M, N= A.shape\n",
    "    # Image dans le domaine fréquentiel\n",
    "    AA = np.fft.fftshift(np.fft.fft2(A)) \n",
    "    M0 = np.ceil((M+1) / 2)\n",
    "    N0 = np.ceil((N+1)/2)\n",
    "    U, V = np.mgrid[1:M+1, 1:N+1]\n",
    "    D2 = (U - M0)**2 + (V - N0)**2\n",
    "    # Réponse fréquentielle du filtre gaussien\n",
    "    HH = np.exp(-D2 / (2 * fc**2))\n",
    "    # Application du filtre et retour au domaine spatial\n",
    "    BB = np.fft.ifftshift(AA*HH) \n",
    "    B = np.fft.ifft2(BB)\n",
    "    B = np.real(B)\n",
    "    return B\n",
    "\n",
    "def filtpb_butter(A, fc, ordre): \n",
    "    M,N=A.shape\n",
    "    # Image dans le domaine fréquentiel\n",
    "    AA = np.fft.fftshift(np.fft.fft2(A)) \n",
    "    M0 = np.ceil((M+1) / 2)\n",
    "    N0 = np.ceil((N+1) / 2)\n",
    "    U, V = np.mgrid[1:M+1, 1:N+1]\n",
    "    D2 = (U - M0)**2 + (V - N0)**2\n",
    "# Réponse fréquentielle du filtre Butterworth\n",
    "    HH = 1 / (1 + (D2 / fc**2)**ordre)\n",
    "# Application du filtre et retour au domaine spatial\n",
    "    BB = np.fft.ifftshift(AA * HH) \n",
    "    B = np.fft.ifft2(BB)\n",
    "    B = np.real(B)\n",
    "    return B\n",
    " \n",
    "\n",
    "#%matplotlib inline plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "dpiGlobal = 100\n",
    "def sliders_gauss(image,a=1, fc=40):\n",
    "    pb_gauss = filtpb_gauss(image, fc)\n",
    "    img_gauss = image + a*(image-pb_gauss)\n",
    "    #plt.figure(dpi=dpiGlobal)\n",
    "    return img_gauss\n",
    "#slider1 = interactive(sliders_gauss, a=(1, 3, 0.5), fc=(1, 50, 5)) \n",
    "#display(slider1)\n",
    "def sliders_butter(image,a=3, fc=46, ordre=2):\n",
    "    pb_butter = filtpb_butter(image, fc, ordre)\n",
    "    img_butter = image + a*(image-pb_butter)\n",
    "    #plt.figure(dpi=dpiGlobal)\n",
    "    return img_butter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path2 = []\n",
    "last_label2 = []\n",
    "for i in range(len(img_name)):\n",
    "    \n",
    "    img2 = cv2.imread(img_name[i], 0)\n",
    "    img2 = cv2.resize(img, (224,224))\n",
    "    rows, cols= img2.shape\n",
    "    img2=sliders_butter(img2)\n",
    "    for angle in range(360):\n",
    "            M2 = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)    #Rotate 0 degree\n",
    "            img_rotated2 = cv2.warpAffine(img2, M2, (224, 224))\n",
    "            img_path2.append(img_rotated2)\n",
    "            if label[i] == 1:\n",
    "                last_label2.append(1)\n",
    "            else:\n",
    "                last_label2.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un nouveau modèle utilisant les images filtrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(img_path2, last_label2, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = np.array(x_train2)\n",
    "x_test2 = np.array(x_test2)\n",
    "(a,b,c)=x_train2.shape # (35136, 224, 224)\n",
    "x_train2 = np.reshape(x_train2, (a, b, c, 1)) # 1 for gray scale\n",
    "(a, b, c)=x_test2.shape\n",
    "x_test2 = np.reshape(x_test2, (a, b, c, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model()\n",
    "model2.summary()\n",
    "saveModel(model3,save3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "datagen.fit(x_train)\n",
    "hist2 = model2.fit(datagen.flow(x_train2, y_train2, batch_size=32,\n",
    "                 subset='training'),\n",
    "                 steps_per_epoch = len(x_train2)/32,\n",
    "                 validation_split=0.3,\n",
    "                 epochs=5,\n",
    "                 batch_size=8,\n",
    "                 callbacks=[early_stop, model_check_point],validation_data=x_test2,y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value2 , accuracy2 = model2.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test_loss_value = ' +str(loss_value2))\n",
    "print('test_accuracy = ' + str(accuracy2))\n",
    "\n",
    "print(model.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result2 = Visualize(hist2.history['accuracy'],hist2.history['val_accuracy'], hist2.history['loss'], hist2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path3 = []\n",
    "last_label3 = []\n",
    "for i in range(len(img_name)):\n",
    "    \n",
    "    img3 = cv2.imread(img_name[i], 0)\n",
    "    img3 = cv2.resize(img3, (224,224))\n",
    "    rows, cols= img3.shape\n",
    "    img2=sliders_gauss(img3)\n",
    "    for angle in range(360):\n",
    "            M3 = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)    #Rotate 0 degree\n",
    "            img_rotated3 = cv2.warpAffine(img3, M3, (224, 224))\n",
    "            img_path3.append(img_rotated3)\n",
    "            if label[i] == 1:\n",
    "                last_label3.append(1)\n",
    "            else:\n",
    "                last_label3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(img_path3, last_label3, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3 = np.array(x_train3)\n",
    "x_test3 = np.array(x_test3)\n",
    "(a,b,c)=x_train3.shape # (35136, 224, 224)\n",
    "x_train3 = np.reshape(x_train3, (a, b, c, 1)) # 1 for gray scale\n",
    "(a, b, c)=x_test3.shape\n",
    "x_test3 = np.reshape(x_test3, (a, b, c, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = model()\n",
    "model3.summary()\n",
    "saveModel(model3,save3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "hist3 = model3.fit(datagen.flow(x_train2, y_train2, batch_size=32,\n",
    "                 subset='training'),\n",
    "                 steps_per_epoch = len(x_train2)/32,\n",
    "                 validation_split=0.3,\n",
    "                 epochs=5,\n",
    "                 batch_size=8,\n",
    "                 callbacks=[early_stop, model_check_point],validation_data=x_test3,y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value3 , accuracy3 = model3.evaluate(x_test3, y_test3)\n",
    "\n",
    "print('Test_loss_value = ' +str(loss_value3))\n",
    "print('test_accuracy = ' + str(accuracy3))\n",
    "\n",
    "print(model.predict(x_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result3 = Visualize(hist2.history['accuracy'],hist2.history['val_accuracy'], hist2.history['loss'], hist2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
